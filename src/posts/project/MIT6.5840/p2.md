---
date: 2024-09-05
category:
  - 数据库
  - 分布式
tag:
  - go
footer: 凉了的馒头
---


# 6.5840 Lab 2: Raft


## 前言  

开学之后杂事就太多了，好不容易抽了段时间写完了 Raft~

Raft 作为著名的分布式共识算法，在分布式领域有着举足轻重的地位。这次的 lab 我们就要亲手实现一个 Raft。

我个人在写的时候没有 **借鉴任何代码乃至开源实现。** 所以提供的思路可能有一些漏洞？以下的思路基本都是我个人的思考，我计划写以下的东西

- 一些论文没提到的地方
- raft 有的地方为什么要这样设计
- 一些代码编写时的设计思路

做实验前要做好花费时间 debug 的准备。如果我们在测试时出现 bug，大部分情况都是对 Raft 理解不够导致写出了错误的代码。而因为论文中省略了一些细节，所以如果没有理解的话，是难以将代码写对的。我推荐反复看论文来提高自己的理解。


## 个人在写的时候参考过的资料  

- Raft 论文，看十几遍也不为过
- Raft 动画，强推 [Raft-Animation](http://thesecretlivesofdata.com/raft/)
- 官方文档 [6.5840 Lab 2: Raft (mit.edu)](https://pdos.csail.mit.edu/6.824/labs/lab-raft.html)
- go 语法：我个人是结合官网，自己写小函数来学习

我阅读的资料不是太多，主要都是自己看论文理解。我觉得论文足够让你写完这个 Lab。


## 2A  

2A 大致就是将论文中的接口完善一下，然而其中也涉及一些细节值得被注意。

### 1. 如何实现 Election timeout?  

比如，我们在给一个server发心跳时，如何重置计时器等等。一开始我一直纠结于如何让计时器暂停，但后来发现 guidance 里提到让**选举的计时和心跳的计时分开**就恍然大悟了。这里我的做法是用一个 chan 来同步心跳的计时，它实际起到条件变量的作用：

其中 `rf.heartBeat` 是 `chan bool` 。每次 AppendEntry 或者 RequestVote 中添加 `rf.heartBeat<-true` 即可:

![](/assets/posts/MIT6.5840-Refs/1.png)  

这里的逻辑用 Timer 应该也可以完成。

### 2. leader 是否需要给自己发送心跳？何时发送？  

应该是都可以。我的设计里面为了让 leader 快速退化，leader 会给自己也发送心跳。

何时给自己发送心跳？**在每次成功地`rf.sendAppendEntry()`之后发送**，这样可以保证 leader 断连后其正确地降级。 一个断连的 leader 是不会成功地发送消息的，从而退化为 Follower。

如果不发送当然也可以，同一时间可能存在两个 leader，从而出现网络分区，但不会对正确性有任何影响。

### 3.关于何时使用并发  

写的时候关注论文，比如论文在阐述 Candidate 发起选举后要做什么时有这样一句话：

> It then votes for itself and issues RequestVote RPCs **in parallel** to each of the other servers in the cluster.

这里就提示了我们要起多个协程来异步地 RequestVote，而不是在同一个协程下 RequestVote 。否则如果有一个节点失效了，整个选举就被卡在了那个地方。同理，别的 RPC 也都是并发地进行着。

### 4. RequestVote 的奇怪设计  

![](/assets/posts/MIT6.5840-Refs/2.png)  

如果第一次实现的时候，难免会觉得为什么 **voteFor 会等于 candidateID** 呢？这不是冗余设计吗，一个任期内怎么可能对同一个 follower 发起两次 RequestVote？在后续的对 RPC 的思考中我找到了答案：事实上，网络延迟会导致 Raft 重传 RPC，所以即使同一任期也可能 leader 对一个 follower 发出两次相同的 RequestVote RPC。而在 Raft 中 RPC 应该是**幂等**的，每一次结果的请求应该返回相同的结果，所以这样设计是为了保证返回幂等性。

当然这对我们的编程实现造成了影响，**我们不能仅仅维护一个原子变量来统计票数了**。或许应该维护一个得票数组。当然我这里为了简单就没有按照论文的来做，而是直接去掉了对 candidateID 的判断，因为我想不到这对整体的设计有任何的影响。并且我的设计里也不会重传 RequestVote RPC

### 5. 为什么要用锁？  

最常见的冲突应该是 DATA RACE，有一个协程读了一个变量而那个变量被另一个协程写了。这里的问题是读写顺序是不固定的，我们需要用锁来使协程独占式地访问。比如，`rf.currentTerm`, `rf.identity` 等等都是需要保护的。又比如我们也不希望两个 AppendEntry 同时运行 ，这很可能造成日志的覆盖。

如何加锁参见[官方tip](https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt)，基本说清了会出现的问题。当我们加完该加的锁后，运行`go test race`来检验是否有冲突。而对于 `rf.me` 这样的只读变量则无需用锁。

同时，RPC的时候肯定是不能持锁的

### 6. 可能会用到的 go 语法  

- 在 go 中，可以使用 `defer rf.mu.Unlock()` 来解锁

在一个函数中的 defer 可以看作一个栈，在函数结束后会依次执行这个栈中的函数。

```go
func main() {
	tryDefer()
}
func tryDefer() {
	defer func() { println("A") }()
	defer func() { println("B") }()
	defer func() { println("C") }()
	println("D")
}

```

输出 DCBA

然而我们一定要在 RPC 之前解锁，RPC 期间不能持锁。所以千万不能每个函数不加思考地用 defer 。像 AppendEntry 和 RequestVote 这样的就可以直接用。

- 原子变量如何使用

原子变量虽然很容易用错，但为了 votes 单开一把锁又很不划算。

```go
var votes uint32 = 1
...
atomic.AddUint32(&votes, 1)
if v := atomic.LoadUint32(&votes); v > (uint32(len(rf.peers) / 2)) {
	rf.becomeLeader()
}

```

像上面这样的就是错误的用法，add 和 Load 之间没有任何的保证。

```go
var votes uint32 = 1
...
if if v := atomic.AddUint32(&votes, 1); v > (uint32(len(rf.peers) / 2)) {
	rf.becomeLeader()
}

```

- 条件变量如何使用

在 go 中我发现完全可以使用 chan 来代替条件变量作用，我也正是这样做的。使用通道：`ch := make(chan bool, 15)` 即可。

### 参考2A结果  

![](/assets/posts/MIT6.5840-Refs/3.png)  

## 2B  

### 1. 如何 Debug？  

官方提供了一个建议

[Debugging by Pretty Printing (josejg.com)](https://blog.josejg.com/debugging-pretty/)

在 linux 下

```shell
ln -s /usr/bin/python3 /usr/bin/python
sudo apt install python3-pip
python -m pip install typer
python -m pip install rich 
```

随后就可以使用了

```shell
go test -run TestSnapShotInit2D > log.txt
python3 dslog.py log.txt -c 3
```

最后的效果大概是这样：

![](/assets/posts/MIT6.5840-Refs/4.png) 

### 2. log 索引从 1 开始  

我一开始论文看的不仔细，加了一堆特判，如果日志为空怎么怎么样。然而，只需要在第一个位置加上一个空日志就可以完美避免这些特判。

![](/assets/posts/MIT6.5840-Refs/5.png)  

### 3. 心跳和 AppendEntry 日志同步的逻辑合二为一  

一开始我的设计如论文所写，空白的心跳一个函数，日志同步一个函数。然而随着逻辑的渐渐相同，合二为一的感觉简直是呼之欲出了。事实上，论文里也提到了这一点。

### 4. 请求过期了  

每一次 RPC 过后，需要检查 rf.CurrentTerm == args.Term 防止 RPC 已经过期。对于过期的请求，直接 Return 即可

### 5. 两个节点的日志谁更新？为什么这样比较？  

![](/assets/posts/MIT6.5840-Refs/6.png)  


- 先比最后一条日志的 Term 再比日志的长度
- 在这样的共识算法中，我们需要让所有节点达成一致的状态。所以我们需要对所有的 Entry 标定某种**全序关系。** 在 Raft 中，Entry 可以看作 <Term, Index> 对。由于 Term, Index 都是递增的，所以一定会形成全序。
- 对于两个节点的日志，最后一条更新则节点的日志更新。所以我们对最后一条 Entry 的 Term 与 Index 进行比较。

### 6. 无限重传？  

在论文 5.5 中提到如果 RPC 发送没有响应，要一直重复发送。我个人的代码原来也是这样做的，将 RPC 改为异步的，通过 chan 传递结果。最后导致 RPC 特别多，而并没有什么好的结果。

**所以在 test 中我认为只需要一次 RPC即可**。同时我后来思考了一下，RPC 大多基于 TCP，难道传递失败不应该是由**传输层来负责重传**吗？否则就说明是更上层出错了。但是对于 AppendEntry 其本身就在周期性的重复；对于 RequestVote，就算失败了一个节点，只要多个节点正常也可以正常地进行下去，根本没有多次重传的必要。

最后我的所有 RPC 都只传递一次。

### 7. 将 Debug 粒度放小  

通过观察正确的日志往往也可以找到错误的原因。

比如我将日志粒度放到锁的级别，发现 RequestVote 的函数时间跨度较长：

```shell
Test (2B): basic agreement ...
004835 TRCE S2 HOLD LOCK IN startElection
004835 TERM F2 Starts an ELECTION (T1)
004835 TRCE S2 RELEASE LOCK IN startElection
004838 VOTE F1 RECEIVE REQUEST VOTE FROM C2 CT1
004838 TRCE S1 HOLD LOCK IN RequestVote
004838 VOTE F1 -> C2 GIVE VOTE (T1)
004838 VOTE F0 RECEIVE REQUEST VOTE FROM C2 CT1
004838 TRCE S0 HOLD LOCK IN RequestVote
005115 VOTE F0 -> C2 GIVE VOTE (T1)
005122 PERS S0 SAVE State: T:1 V:2 LLI:0
005123 TRCE S0 RELEASE LOCK IN RequestVote
005122 PERS S1 SAVE State: T:1 V:2 LLI:0
005123 TRCE S1 RELEASE LOCK IN RequestVote
```

花了 30ms，这是很夸张的，理论上应该在$\mu s$ 级别

这就是**在正确中看到了不正确**，后来通过将内部 chan 移出锁外，时间果然恢复正常：

```shell
003791 TRCE S0 HOLD LOCK IN startElection
003791 TERM F0 Starts an ELECTION (T1)
003791 TRCE S0 RELEASE LOCK IN startElection
003795 VOTE F1 RECEIVE REQUEST VOTE FROM C0 CT1
003795 TRCE S1 HOLD LOCK IN RequestVote
003795 VOTE F1 -> C0 GIVE VOTE (T1)
003795 PERS S1 SAVE State: T:1 V:0 LLI:0
003796 TRCE S1 RELEASE LOCK IN RequestVote
```

**这也是 lab 的难点之一，即使 test 正确，程序仍然会有非常隐晦的漏洞，它们看似与正确性无关，却影响着时间，进而可能导致程序出错。比如这里我的 chan 放在函数内部或外部都是正确的，但放在内部无形中拖慢了函数时间，我却意识不到。**

  


### 参考2B结果  

![](/assets/posts/MIT6.5840-Refs/7.png)  

## 2C  

这部分课程都把持久化的代码写差不多了，照着改一下就好了。对照着 Figure5.2，我们需要持久化 `CurrentTerm` `VoteFor` `Log`随后在这三者改变的地方全部加上 `rf.persist()`即可。随后就可通过测试。

但这部分测试很可能找出之前没找出的错误，因为 2C 的测试较为困难。比如打出bug后我排查出了 split brain。经检查都是 2B 中一些小的问题。比如：

```go
if rf.CurrentTerm > args.Term {
	return
}
if prevLogNotMatch() {
    ...
    rf.persist()
    return
}
if rf.ifTermFallBehind(args.Term) {
    rf.persist()
}

```

这里我的任期更新太迟了，应该先检查任期，再匹配 prevLog。其实原来写 2A 的时候意识到要立马更新任期，然而在后续写 2B 的时候完全忘了这回事，直接在中间插入了一段逻辑，导致了错误。

还有造成 split brain 的一个原因是每一次 RPC 结束后都没有判断身份，比如 sendRequest 后要判断是否已经成了 Follower。因为由于网络的原因可能两秒才收到选票，而此时已经有别的 leader了，自己已经是 Follower了。类似的每一个 RPC 后我们都需要判断身份。

```go
// The request lost due to the network. / The server is down.
if rf.sendRequestVote(index, args, reply) {
    // When net is blocked, after sendRequestVote it may already be a Follower.			
    // If we don't check the identity, there will be a split brain.
        if rf.isFollower() {
            return
        }
...
}

```

### 1. 网络的影响  

Raft 协议对**网络的要求很高**，如果网不好，基本提交不了几个 Entry。从测试的结果也可以看出来，协商了很久最后也才提交了100来条。

最主要原因考虑如下场景：网络使 leader 向某个节点同步日志时卡住，导致那个节点无法收到心跳而开始选举。等一堆日志到达那个节点后会发现自己的任期没它大，从而浪费了这次 RPC。同时，leader 会因此退化为 Follower，而leader很多的日志可能都是未提交的，此时损失是很惨重的。这意味着在这几秒内客户的很多请求是会被抛弃的，而这大大降低了效率。很多之前的 appendEntry 都因为没有提交而会被覆盖。

### 2. 一个优化  

这部分还需要完成论文中 5.3 最后的优化。在 Figure8(unreliable) 测试中，rpc 会模拟网络堵塞的情况，每一个请求可能 2 秒之后才会产生结果。这时如果没有这个优化，因为 nextIndex 一个一个降得太慢，而每一次都可能触发一次堵塞的 rpc，所以很可能无法达成一致通不过测试。幸运的是课程中已经告诉我们该如何做这个优化：

```
  Case 1: leader doesn't have XTerm:
    nextIndex = XIndex
  Case 2: leader has XTerm:
    nextIndex = leader's last entry for XTerm
  Case 3: follower's log is too short:
    nextIndex = XLen
```

![](/assets/posts/MIT6.5840-Refs/8.png)  

### 3. 进一步的优化  

观察 Log 发现如果 leader 对一个 follower 的网堵塞了一会，那么那个 follower 将提高 Term 造成要重新选举。如果此时的 leader 仍可以和大部分节点通信，重新选举是很浪费的。

在网上搜索后发现作者已经提到过这个优化的解法了：

[https://zhuanlan.zhihu.com/p/35697913](https://zhuanlan.zhihu.com/p/35697913)

**一种类似于 2PC 的二阶段选举**

**当然因为比较复杂我并没有实现它**

### 4. 是否需要持久化更多的变量？比如: commitIndex  

从直觉上来说 commitIndex 很重要。然而我们有一个优化，在选举的开始时加一条空日志。这可以快速提升 matchIndex，从而快速恢复 commitIndex。当然在这个项目里不能这样优化，否则 Test2B 不过。不过这说明了我们没有必要持久化 commitIndex。

[为什么 Raft 的 ApplyIndex 和 CommitIndex 不需要持久化？](https://www.zhihu.com/question/382888510/answer/1214165091)

### 5. matchIndex 有什么用  

matchIndex 唯一的作用就是提高 commitIndex。这也是**唯一**提高 commitIndex 的手段。如果你有两个地方可以提高 commitIndex，说明你需要合并你的逻辑。matchIndex 仅在成功同步日志后需要被更新。还有一个细节很容易引发 bug：

- 在 becomeLeader 后不要忘记更新自己本身的 matchIndex： `rf.matchIndex[rf.me] = rf.getLastLogIndex()`
- 在 Start 后不要忘记更新自己本身的 matchIndex：`rf.matchIndex[rf.me] = LogIndex`

![](/assets/posts/MIT6.5840-Refs/9.png)  

同时这里有一个问题。考虑这样一个场景：当 leader start 一个 entry 后，leader 将 entry 加入自己的日志。随后一个之前断连的 Follower 回归，因为 Follower 一直在选举，所以 Term 更大 会导致 leader 暂停日志的同步。但在之后的选举中它仍然会是 leader 因为它有最新的日志。但这个日志无法被提交，因为它是过期的日志。所以这就造成了一个困境，我们必须要在当前任期内再有人 start 一次，将新的 entry 提交，**顺便把旧的也提交了**。这就是一直强调空白日志的作用。

在阅读 test 的时候，会发现写 test 的人是知道这个问题的。但是为了保证对我们提交日志的检查，其还是不允许我们提交空日志。他的解决方法是**在每个 test 的最后都通过 cfg.one() 发起一次日志**，从而让 Raft 提交所有日志达成共识。

### 6. 理解 Figure8  

![](/assets/posts/MIT6.5840-Refs/10.png)  

- 记 Entry 为 \<Term, Index\>
- S1 加入两条 \<2, 2\> 后挂掉
- S5 得到 S3，S4的票成为 leader，加入 \<3, 2\> 后挂掉
- S1 拿到S2，S3，S4 的票成为 leader，加入 \<4, 3\>后挂掉
- **假设我们允许 commit 过期的日志，那么 \<2, 2\> 会被 commit**
- 随后 S5拿到S2，S3，S4 的票成为 leader，同步 \<3, 2\>
- \<2, 2\>被覆盖掉，但它已经被 commit ，所以错误。

**综上 Raft 不允许 commit 过期的日志**

### 7. 只有一把锁的死锁  

查看 log，发现有的 server 跑着跑着人没了，导致最后 Term 都快到五位数了。我百思不得其解，只要上锁与解锁一一对应，一把锁怎么会死锁？

思考了很久，挣扎着才想到。在学习 go 的时候有看过这么一句话：**chan 是线程安全的**。所以 chan 也就意味着隐含一把锁。翻看 go 的源码，果然如此。这样死锁的产生也就说的通了。最后将 chan 移出到锁的外面，死锁现象消失。

后来详细的了解了下 chan 的机制。无缓冲区的 chan 是同步的，所以极易造成死锁。

### 8. 用脚本测试  

官方提供了网站

[Debug](https://blog.josejg.com/debugging-pretty/)

在 linux 下

```shell
ln -s /usr/bin/python3 /usr/bin/python
sudo apt install python3-pip
python -m pip install typer
python -m pip install rich 
```

随后即可运行其提供的脚本

比如：`python3 dstest.py TestFigure8Unreliable2C -n 300 -p 8`

### 9. 测试多少次合适？一个逆天的 bug  

我个人觉得至少需要 1000 次，在测试的过程中将充分见识到问题的多种多样。

比如：

- 排查问题到最后，发现是系统不靠谱

```go
t = time.Now()
time.Sleep(heartBeatInterval())
if time.Since(t) > heartBeatInterval()*3/2 {
	Debug(dError, "L%d Time Sleep Error %v", rf.me, time.Since(t))
	os.Exit(11)
}

```

我在加入了上面的语句后 10 个测试有 5 个错的。也就是说 `time.Sleep()` 的误差极大。我排查这个问题排查了一下午，从来没想过 `time.Sleep()` 能有那么大的误差。误差每次都在 1ms 上下，但会突然有一两次误差特别大。显然这涉及到 golang 的底层实现的问题了。

我觉得问题主要来自于 goroutine 多了之后，在切换时导致了时间的管理出现了问题。

![](/assets/posts/MIT6.5840-Refs/11.png)  

最逆天的，一个 100ms 的 Sleep，Sleep 了 1.4s。表现上就是 Leader 会突然不发出心跳导致成为 Follower。

**在实际生产中这样没有问题，因为最多换个 Leader不影响什么** ，但测试不行。不行的原因**依然是不支持空日志，导致无法提交过期的日志，而测试又不会提供新日志来提高 commitIndex**。

而引入空日志在这里并不可行，可自行尝试便知其中难点。

这个问题暂时对于我来说是无解的。我几千次测试就会出现一次这个问题。

总的来说有三个解决办法：

- 将心跳时间设置到 1.3s ，这样后面有的测试过不了
- 空日志，同样有测试过不了
- 找到代码级的原因，并解决之。然而这个难度太大了，是虚拟机时间同步的锅？还是协程调度的锅？就算找到了，又怎么修改呢？除了使用 `time.Sleep()` 没有别的办法暂停一段时间。那这个问题就难以解决。

在stackoverflow上找到同样的疑问:

[https://stackoverflow.com/questions/73384457/go-unexpected-results-from-time-sleep](https://stackoverflow.com/questions/73384457/go-unexpected-results-from-time-sleep)

造成这个问题的另一个原因很可能是虚拟机。我用Vmware和Wsl均会出现这个问题。

**2024.06.13** 为了进一步确定，我从网上随机找到了一位作者的实现，它的实现通过了10000次测试，按理说应该没有问题。然而在我的机器上，180次测试就喜提一个fail。这让我确定了就算实现正确，测试也可能无法通过的信念。这里仍然需要强调：测试不通过不能代表 raft 实现不正确。于是，我释然了。

  


![](/assets/posts/MIT6.5840-Refs/12.png)  


**2024.11.22** 将代码放到服务器上跑了一下，很轻松地一万次没错误通过。盲猜是虚拟机的锅

![](/assets/posts/MIT6.5840-Refs/13.png)  


## 2D  

2D 是关于快照的。这里 lab 中说的不是很清楚，我是通过看 test 才对快照有了更好的理解。

### 1. SnapShot 干了什么？  

![](/assets/posts/MIT6.5840-Refs/14.png)  

注释说的很清楚，SnapShot 是上层应用调用的。在这个项目里，test 会定期调用 SnapShot, 这里的形参 `snapshot []byte` 是上层应用替我们打包好的，**不需要我们自己编码生成**

它做的就是将日志打包到指定的 Index，随后持久化之。

因为日志不能为空，所以自然地想到将每一次的快照的最后一条日志放到第一个的位置。之后每次需要上一次快照的 lastIncludedIndex lastIncludedTerm 的地方直接取第一个的 Index Term 即可

### 2. Crash 后的恢复  

当一个节点 crash 后，其将 `readPersist()` 来进行恢复。试想一个 KV 数据库，当我们进行多次 SET 操作后 crash，我们需要重新 apply 之前的所有操作从而进行状态的恢复。所以 2C 中的 `readPersist()`需要进行一定的更改。

![](/assets/posts/MIT6.5840-Refs/15.png)  

我们从磁盘中拿出快照，随后更改 `lastAppied`, `commitIndex`

这里会发现我并没有 apply 这个快照，这与上面的逻辑是相悖的。这是因为 test 中替我们做了。

![](/assets/posts/MIT6.5840-Refs/16.png)  

可以看到注释也提到了

> ideally Raft should send it up on applyCh...

### 3. InstallSnapShot 干什么？  

- 试想一个 leader commit 一个日志后进行快照。那么 leader 就失去了所有 log。但它还需要同步这条 log。所以此时它通过给别的节点发送 `InstallSnapShot` 进行同步。
- leader 成功发送快照后，不要忘记更新 `nextIndex`
- Follower 收到 Leader 的快照后，检查是否是最新的快照，如果是就将其直接 apply。因为快照一定是已经 commit 的。随后更新自己的`commitIndex`, `lastApplied`

### 4. applyCh 的时候可以持锁吗？  

### applyCh 的时候不能持锁，否则会死锁  


## 下一步干什么  

当写完 Raft 后，还能看些什么？

- Zab

在课程中，我们学习了 ZooKeeper 的设计。而 ZooKeeper 底层使用Zab进行广播。所以可以顺便看一下 Zab 的设计。

- Paxos

作为最经典的共识算法，值得一看。

[paxos-simple.pdf (lamport.azurewebsites.net)](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)

- DDIA 第九章

[简介 · ddia-cn (gitbooks.io)](https://vonng.gitbooks.io/ddia-cn/content/)

当你阅读完 Zab 和 Paxos，一定会觉得 Raft 是最简单的。

- 看完 Paxos 的理论证明，完全不知道该怎么实现它
- 看完 Zab，会被其庞杂的 RPC 震撼

回头看 Raft，多项功能都被很好的集成在了 RPC 中。最后仅有三种 RPC。Figure 5.2 简明扼要地写出了所有该有的变量以及大部分的逻辑，非常方便理解。


## 最后  

在图书馆测了3000次后

![](/assets/posts/MIT6.5840-Refs/17.png)  

回宿舍又测了5000次

![](/assets/posts/MIT6.5840-Refs/18.png)  

<del>在正确性上应该是有很好的保证了。然而 `time.Sleep()` 突然不准的问题还是一个未解之谜....</del>

